{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from utils.model_utils import (\n",
    "    gen_conv,\n",
    "    lrelu,\n",
    "    batch_norm,\n",
    "    gen_deconv,\n",
    "    discrim_conv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "Model = namedtuple(\n",
    "    'Model',\n",
    "    ['outputs',\n",
    "     'predict_real',\n",
    "     'predict_fake',\n",
    "     'discrim_loss',\n",
    "     'discrim_grads_and_vars',\n",
    "     'gen_loss_GAN',\n",
    "     'gen_loss_L1',\n",
    "     'gen_grads_and_vars',\n",
    "     'train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inputs, out_channels, n_filters):\n",
    "        layers = []\n",
    "\n",
    "        # encode [batch_size, 256, 256, input_chann] =>\n",
    "        #        [batch_size, 128, 128, ngf]\n",
    "        with tf.variable_scope('encode_1'):\n",
    "            output = gen_conv(batch_input=inputs, out_channels=n_filters)\n",
    "            layers.append(output)\n",
    "\n",
    "        layers_specs = [\n",
    "            n_filters * 2,\n",
    "            n_filters * 4,\n",
    "            n_filters * 8,\n",
    "            n_filters * 8,\n",
    "            n_filters * 8,\n",
    "            n_filters * 8,\n",
    "            n_filters * 8\n",
    "        ]\n",
    "\n",
    "        for output_channels in layers_specs:\n",
    "            encoder_name = \"encoder_{}\".format(len(layers) + 1)\n",
    "            with tf.variable_scope(encoder_name):\n",
    "                rectified_inputs = lrelu(layers[-1], 0.2)\n",
    "                convolved = gen_conv(rectified_inputs, output_channels)\n",
    "                output = batch_norm(convolved)\n",
    "                layers.append(output)\n",
    "\n",
    "        layers_specs = [\n",
    "            (n_filters * 8, 0.5),\n",
    "            (n_filters * 8, 0.5),\n",
    "            (n_filters * 8, 0.5),\n",
    "            (n_filters * 8, 0.0),\n",
    "            (n_filters * 4, 0.0),\n",
    "            (n_filters * 2, 0.0),\n",
    "            (n_filters, 0.0),\n",
    "        ]\n",
    "\n",
    "        num_encoder_layers = len(layers)\n",
    "        for dec_layer, (output_channels, dropout) in enumerate(layers_specs):\n",
    "            skip_layer = num_encoder_layers - dec_layer - 1\n",
    "            decoder_name = \"decoder_{}\".format(dec_layer + 1)\n",
    "            with tf.variable_scope(decoder_name):\n",
    "                if dec_layer == 0:\n",
    "                    # no skip connections for the first layer\n",
    "                    inputs = layers[-1]\n",
    "                else:\n",
    "                    inputs = tf.concat(\n",
    "                        [layers[-1], layers[skip_layer]],\n",
    "                        axis=3\n",
    "                    )\n",
    "\n",
    "            rectified_inputs = tf.nn.relu(inputs)\n",
    "            output = gen_deconv(rectified_inputs, output_channels)\n",
    "            output = batch_norm(output)\n",
    "\n",
    "            if dropout > 0.0:\n",
    "                output = tf.nn.dropout(output, keep_prob=(1 - dropout))\n",
    "\n",
    "            layers.append(output)\n",
    "\n",
    "        # decoder_1: [batch_size, 128, 128, n_filters * 2] =>\n",
    "        #            [batch_size, 256, 256, out_channels]\n",
    "        with tf.variable_scope('decoder_1'):\n",
    "            inputs = tf.concat([layers[-1], layers[0]], axis=3)\n",
    "            rectified_inputs = tf.nn.relu(inputs)\n",
    "            output = gen_deconv(rectified_inputs, out_channels)\n",
    "            output = tf.nn.tanh(output)\n",
    "            layers.append(output)\n",
    "\n",
    "        return layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(predict_fake,\n",
    "                   targets,\n",
    "                   generated,\n",
    "                   gan_weight=1.0,\n",
    "                   l1_weight=100):\n",
    "    with tf.variable_scope('generator_loss'):\n",
    "        # predict_fake => 1\n",
    "        # abs(targets - outputs) => 0\n",
    "        gen_loss_gan = tf.reduce_mean(-tf.log(predict_fake + EPS))\n",
    "        gen_loss_l1 = tf.reduce_mean(tf.abs(targets - generated))\n",
    "        gen_loss = gen_loss_gan * gan_weight + gen_loss_l1 * l1_weight\n",
    "        return gen_loss, gen_loss_l1, gen_loss_gan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, targets, n_filters):\n",
    "    n_layers = 3\n",
    "    layers = []\n",
    "    # 2x [batch, height, width, in_channels] =>\n",
    "    #    [batch, height, width, in_channels * 2]\n",
    "    inputs = tf.concat([inputs, targets], axis=3)\n",
    "\n",
    "    with tf.variable_scope('layer_1'):\n",
    "        convolved = discrim_conv(inputs, n_filters, stride=2)\n",
    "        rectified_output = lrelu(convolved, 0.2)\n",
    "        layers.append(rectified_output)\n",
    "\n",
    "    # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\n",
    "    # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\n",
    "    # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\n",
    "    for i in range(n_layers):\n",
    "        layer_name = 'layer_{}'.format(len(layers) + 1)\n",
    "        with tf.variable_scope(layer_name):\n",
    "            output_channels = n_filters * min(2 ** (i + 1), 8)\n",
    "            stride = 1 if i == n_layers - 1 else 2\n",
    "            convolved = discrim_conv(layers[-1], output_channels, stride)\n",
    "            normalized = batch_norm(convolved)\n",
    "            rectified_output = lrelu(normalized, 0.2)\n",
    "            layers.append(rectified_output)\n",
    "\n",
    "    layer_name = 'layer_{}'.format(len(layers) + 1)\n",
    "    with tf.variable_scope(layer_name):\n",
    "        convolved = discrim_conv(layers[-1], out_channels=1, stride=1)\n",
    "        output = tf.nn.sigmoid(convolved)\n",
    "        layers.append(output)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(predict_real, predict_fake):\n",
    "    with tf.variable_scope('discriminator_loss'):\n",
    "        # minimizing -tf.log will try to get inputs to 1\n",
    "        # predict_real => 1\n",
    "        # predict_fake => 0\n",
    "        return tf.reduce_mean(\n",
    "            -(tf.log(predict_real + EPS) + tf.log(1 - predict_fake + EPS))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputs, targets):\n",
    "    \"\"\"\n",
    "\n",
    "    Create a U-Net model with skip connections\n",
    "\n",
    "    :param inputs: Input images\n",
    "    :type inputs: tensorflow.placeholder\n",
    "    :param targets: targeting images\n",
    "    :type targets: tensorflow.placeholder\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('generator'):\n",
    "        output_channels = int(targets.get_shape()[-1])\n",
    "        generated = generator(inputs, output_channels, 64)\n",
    "\n",
    "    # create two copies of discriminator, one for real pairs\n",
    "    # and one for fake pairs. They share the same underlying variables\n",
    "    with tf.variable_scope('discriminator_real'):\n",
    "        with tf.variable_scope('discriminator'):\n",
    "            # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n",
    "            predict_real = discriminator(inputs, targets, 64)\n",
    "\n",
    "    with tf.variable_scope('discriminator_fake'):\n",
    "        with tf.variable_scope('discriminator', reuse=True):\n",
    "            predict_fake = discriminator(inputs, generated, 64)\n",
    "\n",
    "    discriminator_loss = discriminator_loss(\n",
    "        predict_real,\n",
    "        predict_fake\n",
    "    )\n",
    "    generator_loss, generator_loss_l1, gen_loss_gan = generator_loss(\n",
    "        predict_fake,\n",
    "        targets,\n",
    "        generated\n",
    "    )\n",
    "\n",
    "    with tf.variable_scope('discriminator_train'):\n",
    "        discrim_train_vars = [\n",
    "            var for var in tf.trainable_variables()\n",
    "            if var.name.startswith('discriminator')\n",
    "        ]\n",
    "        discrim_optim = tf.train.AdamOptimizer(0.0002, 0.5)\n",
    "        discrim_grads_vars = discrim_optim.compute_gradients(\n",
    "            discriminator_loss,\n",
    "            var_list=discrim_train_vars\n",
    "        )\n",
    "        discrim_train = discrim_optim.apply_gradients(discrim_grads_vars)\n",
    "\n",
    "    with tf.variable_scope('generator_train'):\n",
    "        with tf.control_dependencies([discrim_train]):\n",
    "            gen_train_vars = [\n",
    "                var for var in tf.trainable_variables()\n",
    "                if var.name.startswith('generator')\n",
    "            ]\n",
    "            gen_optim = tf.train.AdamOptimizer(0.0002, 0.5)\n",
    "            gen_grads_vards = gen_optim.compute_gradients(\n",
    "                generator_loss,\n",
    "                var_list=gen_train_vars\n",
    "            )\n",
    "            gen_train = gen_optim.apply_gradients(gen_grads_vards)\n",
    "\n",
    "    exp_moving_average = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "    update_losses = exp_moving_average.apply(\n",
    "        [discriminator_loss, generator_loss, generator_loss_l1]\n",
    "    )\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    incr_global_step = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "    return Model(\n",
    "        predict_real=predict_real,\n",
    "        predict_fake=predict_fake,\n",
    "        discrim_loss=exp_moving_average.average(discriminator_loss),\n",
    "        discrim_grads_and_vars=discrim_grads_vars,\n",
    "        gen_loss_GAN=exp_moving_average.average(gen_loss_gan),\n",
    "        gen_loss_L1=exp_moving_average.average(generator_loss_l1),\n",
    "        gen_grads_and_vars=gen_grads_vards,\n",
    "        outputs=generated,\n",
    "        train=tf.group(update_losses, incr_global_step, gen_train),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(dtype=tf.float32, shape=[None, 256, 256, 3])\n",
    "targets = tf.placeholder(dtype=tf.float32, shape=[None, 256, 256, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('generator'):\n",
    "        output_channels = int(targets.get_shape()[-1])\n",
    "        generated = generator(inputs, output_channels, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('discriminator'):\n",
    "    # 2x [batch, height, width, channels] => [batch, 30, 30, 1]\n",
    "    predict_real = discriminator(inputs, targets, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('discriminator', reuse=True):\n",
    "    predict_fake = discriminator(inputs, generated, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'discriminator_fake/discriminator/layer_5/Sigmoid:0' shape=(?, 30, 30, 1) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'discriminator_real/discriminator/layer_5/Sigmoid:0' shape=(?, 30, 30, 1) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_loss = discriminator_loss(\n",
    "    predict_real,\n",
    "    predict_fake\n",
    ")\n",
    "generator_loss, generator_loss_l1, gen_loss_gan = generator_loss(\n",
    "    predict_fake,\n",
    "    targets,\n",
    "    generated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('discriminator_train'):\n",
    "    discrim_train_vars = [\n",
    "        var for var in tf.trainable_variables()\n",
    "        if var.name.startswith('discriminator')\n",
    "    ]\n",
    "    discrim_optim = tf.train.AdamOptimizer(0.0002, 0.5)\n",
    "    discrim_grads_vars = discrim_optim.compute_gradients(\n",
    "        discriminator_loss,\n",
    "        var_list=discrim_train_vars\n",
    "    )\n",
    "    discrim_train = discrim_optim.apply_gradients(discrim_grads_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('generator_train'):\n",
    "    with tf.control_dependencies([discrim_train]):\n",
    "        gen_train_vars = [\n",
    "            var for var in tf.trainable_variables()\n",
    "            if var.name.startswith('generator')\n",
    "        ]\n",
    "        gen_optim = tf.train.AdamOptimizer(0.0002, 0.5)\n",
    "        gen_grads_vards = gen_optim.compute_gradients(\n",
    "            generator_loss,\n",
    "            var_list=gen_train_vars\n",
    "        )\n",
    "        gen_train = gen_optim.apply_gradients(gen_grads_vards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_moving_average = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "update_losses = exp_moving_average.apply(\n",
    "    [discriminator_loss, generator_loss, generator_loss_l1]\n",
    ")\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "incr_global_step = tf.assign(global_step, global_step + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
